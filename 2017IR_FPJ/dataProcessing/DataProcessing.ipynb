{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /nfs/undergrad/03/b03902024/2017IRFPJ/external/dict.txt.big ...\n",
      "Loading model from cache /tmp/jieba.u01d2a77556a057401286ff132b8bfed4.cache\n",
      "Loading model cost 2.448 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100004 Zh word embeddings are loaded.\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import jieba.analyse\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "import pickle\n",
    "from textblob import TextBlob as tb\n",
    "import math\n",
    "import numpy as np\n",
    "import operator\n",
    "import gensim\n",
    "from hanziconv import HanziConv\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Reshape\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D \n",
    "from keras.layers.pooling import GlobalMaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "def setExternelDicts():\n",
    "    jieba.set_dictionary('./external/dict.txt.big')\n",
    "    jieba.analyse.set_stop_words('./external/stop_words.txt')\n",
    "    jieba.analyse.set_idf_path(\"./external/idf.txt.big\")\n",
    "    \n",
    "def cutSentence(sentences):\n",
    "    return [list(jieba.cut(sentence, cut_all=False)) for sentence in sentences.split(' ')]\n",
    "    \n",
    "def rmStopWords(sentences):\n",
    "    for sentence in sentences:\n",
    "        for term in sentence:\n",
    "            if term in stopwords:\n",
    "                sentence.remove(term)\n",
    "        if sentence == []:\n",
    "            sentences.remove(sentence)\n",
    "    return sentences\n",
    "\n",
    "def processString(string):\n",
    "    return rmStopWords(cutSentence(' '.join(re.findall(r'[\\u4e00-\\u9fff]+', string))))\n",
    "\n",
    "def processTitle(title):\n",
    "    return processString(title)\n",
    "\n",
    "def processContent(content):\n",
    "    return processString(content)\n",
    "\n",
    "def processComment(comments):\n",
    "    for i, comment in enumerate(comments):\n",
    "        comments[i] = processString(comment)\n",
    "    return comments\n",
    "\n",
    "def processTag(tag):\n",
    "    tag_list = []\n",
    "    tags = tag.split(',')\n",
    "    for t in tags:\n",
    "        t = re.findall(r'[\\u4e00-\\u9fff]+', t)\n",
    "        tag_list+=t\n",
    "    return tag_list\n",
    "\n",
    "def processUrl(url):\n",
    "    return re.sub(r'[\\n]', '', url)\n",
    "\n",
    "def processPush(push_list):\n",
    "    return [processString(push) for push in push_list]\n",
    "\n",
    "def processBlogs():\n",
    "    path='/tmp2/ten/new_post_pickles/'\n",
    "    filenames = [f for f in listdir(path)]\n",
    "    blogs = []\n",
    "    for filename in filenames:\n",
    "        doc = pickle.load(open( path+filename, \"rb\" ))\n",
    "        doc['title'] = processTitle(doc['title'])\n",
    "        doc['comment'] = processComment(doc['comment'])\n",
    "        doc['content'] = processContent(doc['content'])\n",
    "        if 'tag' in doc.keys():\n",
    "            doc['tag'] = processTag(doc['tag'])\n",
    "            if doc['tag'] != []:\n",
    "                firstTag = ' '.join(list(jieba.cut(doc['tag'][0], cut_all=False)))\n",
    "\n",
    "                cutIndex = len(doc['content'])\n",
    "                for i in range(len(doc['content'])-1, -1, -1):\n",
    "                    if doc['content'][i] == firstTag:\n",
    "                        cutIndex = i\n",
    "                        break\n",
    "                doc['content'] = doc['content'][:cutIndex]\n",
    "\n",
    "        doc['url'] = processUrl(doc['url'])\n",
    "        blogs.append(doc)\n",
    "    return blogs\n",
    "\n",
    "def processPtts():\n",
    "    path_S='/tmp2/GorsachiusMelanolophus/ptt_posts_new/sponsored/'\n",
    "    path_notS='/tmp2/GorsachiusMelanolophus/ptt_posts_new/no_sponsored/'\n",
    "    \n",
    "    filenames_S = [f for f in listdir(path_S)]\n",
    "    filenames_notS = [f for f in listdir(path_notS)]\n",
    "    ptts = [None]*(len(filenames_S)+len(filenames_notS))\n",
    "    for i, filename in enumerate(filenames_S):\n",
    "        if filename == 'test.py':\n",
    "            continue\n",
    "        doc = pickle.load(open( path_S+filename, \"rb\" ))\n",
    "        doc['href'] = 'https://www.ptt.cc' + doc['href']\n",
    "        doc['title'] = processTitle(doc['title'])\n",
    "        doc['content'] = processContent(doc['content'])\n",
    "        cutIndex = len(doc['content'])\n",
    "        for i in range(len(doc['content'])-1, -1, -1):\n",
    "            if '轉錄' in doc['content'][i]:\n",
    "                cutIndex = i\n",
    "        doc['content'] = doc['content'][:cutIndex]\n",
    "        doc['push_contents'] = processPush(doc['push_contents'])\n",
    "        ptts[int(filename[:-2])] = doc\n",
    "    for i, filename in enumerate(filenames_notS):\n",
    "        doc = pickle.load(open( path_notS+filename, \"rb\" ))\n",
    "        doc['href'] = 'https://www.ptt.cc' + doc['href']\n",
    "        doc['title'] = processTitle(doc['title'])\n",
    "        doc['content'] = processContent(doc['content'])\n",
    "        cutIndex = len(doc['content'])\n",
    "        for i in range(len(doc['content'])-1, -1, -1):\n",
    "            if '轉錄' in doc['content'][i]:\n",
    "                cutIndex = i\n",
    "        doc['content'] = doc['content'][:cutIndex]\n",
    "        doc['push_contents'] = processPush(doc['push_contents'])\n",
    "        ptts[len(filenames_S)+int(filename[:-2])] = doc\n",
    "    return ptts\n",
    "\n",
    "def terms2Vec(terms):\n",
    "    vec = np.zeros(len(embeddings[0]))\n",
    "    for term in terms:\n",
    "        ID = word2id.get(HanziConv.toSimplified(term)) #Problem: Some terms are not pretrained, like '食记','咖哩','捷运'\n",
    "        if ID == None:\n",
    "            vec += embeddings[0]\n",
    "        else:\n",
    "            vec += embeddings[ID]\n",
    "    vec /= len(terms)\n",
    "    return vec\n",
    "\n",
    "def getTrainingData(ptts):\n",
    "    X = np.zeros((len(ptts), max_sentences_num, len(embeddings[0])))\n",
    "    y = []\n",
    "    for i in range(len(ptts)):\n",
    "        for j,terms in enumerate(ptts[i]['content']):\n",
    "            X[i][j] = embeddings[startS]+terms2Vec(terms)+embeddings[endS]\n",
    "        if ptts[i]['isSponsoredPost'] == True:\n",
    "            y.append([1,0])\n",
    "        else:\n",
    "            y.append([0,1])\n",
    "    y = np.asarray(y)\n",
    "    return X, y\n",
    "            \n",
    "def getTestingData(blogs):\n",
    "    X = np.zeros((len(blogs), max_sentences_num, len(embeddings[0])))\n",
    "    y = []\n",
    "    for i in range(len(blogs)):\n",
    "        for j,terms in enumerate(blogs[i]['content']):\n",
    "            X[i][j] = embeddings[startS]+terms2Vec(terms)+embeddings[endS]\n",
    "        X.append(np.asarray(padding(vecIDs)))\n",
    "    return X, y\n",
    "\n",
    "\n",
    "max_sentences_num = 1000\n",
    "setExternelDicts()\n",
    "stopwords = [line.rstrip('\\n') for line in open('./external/stopwords-zh.txt')]\n",
    "blogs = processBlogs() \n",
    "\n",
    "ptts = processPtts()\n",
    "\n",
    "# --------- Load word embedding --------- #\n",
    "words, embeddings = pickle.load(open('/tmp2/eee/polyglot-zh.pkl', 'rb'), encoding='latin1')\n",
    "print ('%d Zh word embeddings are loaded.' % len(words))\n",
    "word2id = { w:i for (i,w) in enumerate(words) }\n",
    "startS = word2id['<S>']\n",
    "endS = word2id['</S>']\n",
    "pad = word2id['<PAD>']\n",
    "maxL = 4776"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "path_S='/tmp2/GorsachiusMelanolophus/ptt_posts_new/sponsored/'\n",
    "path_notS='/tmp2/GorsachiusMelanolophus/ptt_posts_new/no_sponsored/'\n",
    "filenames_S = [f for f in listdir(path_S)]\n",
    "sN = len(filenames_S)\n",
    "filenames_notS = [f for f in listdir(path_notS)]\n",
    "notsN = len(filenames_notS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5603\n",
      "5319\n",
      "4682\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train[:sN]))\n",
    "print(len(X_train[sN:]), len(X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# X_train, y_train = getTrainingData(ptts[:int(0.7*len(ptts))])\n",
    "# X_valid, y_valid = getTrainingData(ptts[int(0.7*len(ptts)):])\n",
    "# S_X, S_y = X_train[:sN], y_train[:sN]\n",
    "# notS_X, notS_y = np.concatenate((X_train[sN:], X_valid)), np.concatenate((y_train[sN:], y_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "S_X, S_y = getTrainingData(ptts[:sN])\n",
    "notS_X, notS_y = getTrainingData(ptts[sN:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10922, 1000, 64) (10922, 2)\n",
      "(4681, 1000, 64) (4681, 2)\n",
      "(5603, 1000, 64) (5603, 2)\n",
      "(10000, 1000, 64) (10000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_valid.shape, y_valid.shape)\n",
    "print(S_X.shape, S_y.shape)\n",
    "print(notS_X.shape, notS_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train_new, y_train_new = np.concatenate((S_X[:int(0.7*len(S_X))], notS_X[:int(0.7*len(notS_X))])), np.concatenate((S_y[:int(0.7*len(S_y))], notS_y[:int(0.7*len(notS_y))]))\n",
    "X_valid_new, y_valid_new = np.concatenate((S_X[int(0.7*len(S_X)):], notS_X[int(0.7*len(notS_X)):])), np.concatenate((S_y[int(0.7*len(S_y)):], notS_y[int(0.7*len(notS_y)):]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10922, 1000, 64) (10922, 2)\n",
      "(4681, 1000, 64) (4681, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_new.shape, y_train_new.shape)\n",
    "print(X_valid_new.shape, y_valid_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pickle.dump([blogs, ptts], open( \"/tmp2/GorsachiusMelanolophus/afterProcessing/newBlogs_newPTTs.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(X_train_new[:int(len(X_train_new)/2)], open( \"/tmp2/GorsachiusMelanolophus/afterProcessing/newBlogs_newPTTs_senAvg_train1.p\", \"wb\" ))\n",
    "pickle.dump(X_train_new[int(len(X_train_new)/2):], open( \"/tmp2/GorsachiusMelanolophus/afterProcessing/newBlogs_newPTTs_senAvg_train2.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pickle.dump([y_train_new, X_valid_new, y_valid_new, embeddings], open( \"/tmp2/GorsachiusMelanolophus/afterProcessing/newBlogs_newPTTs_senAvg_noXtrain.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15604, 8671)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ptts), len(blogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#[blogs, ptts, X_train, y_train, X_valid, y_valid, embeddings] = pickle.load(open(\"/tmp2/GorsachiusMelanolophus/afterProcessing/blogs_ptts_sen_avg_newblogs.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a [[[1 2 3]\n",
      "  [4 5 6]]\n",
      "\n",
      " [[1 2 3]\n",
      "  [4 5 6]]]\n",
      "b [[[1 2 3]\n",
      "  [4 5 6]]\n",
      "\n",
      " [[1 2 3]\n",
      "  [4 5 6]]]\n",
      "(4, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "a = np.asarray([1,2,3,4,5,6,1,2,3,4,5,6])\n",
    "a.resize(2,2,3)\n",
    "b = np.asarray([1,2,3,4,5,6,1,2,3,4,5,6])\n",
    "b.resize(2,2,3)\n",
    "print('a',a)\n",
    "print('b',b)\n",
    "print(np.concatenate((a,b)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print('baseline:', isSponsered_list.count(0)/len(ptts))\n",
    "for word in ['試吃', '廠商', '合作', '邀約', '邀稿', '哇']:\n",
    "    print(word+':', sum([1 for a,b in zip(isSponsered_list, hasWord_list(word)) if a == b]) / len(ptts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sentences = [['first', 'sentence'], ['second', 'sentence']]\n",
    "model = gensim.models.Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -4.60046995e-03,  -7.77644527e-05,  -3.48091591e-03,\n",
       "        -1.45446544e-03,   2.42910930e-03,   6.14786404e-05,\n",
       "         2.99958792e-03,  -1.65786792e-03,  -4.49421583e-03,\n",
       "         2.69324542e-03,  -6.30077775e-05,   3.51313362e-03,\n",
       "        -3.15759680e-03,   1.17169262e-03,  -2.98711169e-03,\n",
       "        -3.37286503e-03,   4.55837278e-03,  -1.77195738e-03,\n",
       "        -1.81711488e-03,   3.88350803e-03,  -1.64524664e-03,\n",
       "         4.25767526e-03,  -1.23760244e-03,  -1.12451136e-03,\n",
       "         4.56460984e-03,   2.12160405e-03,   1.87750976e-03,\n",
       "         1.54259242e-03,   2.26992904e-03,  -4.71394602e-03,\n",
       "         4.97746328e-03,   4.26378427e-03,   4.73562768e-03,\n",
       "        -2.01302324e-03,   6.12006290e-04,   3.53679038e-03,\n",
       "        -3.13258497e-03,   1.69070170e-03,  -1.35397946e-03,\n",
       "        -3.52162635e-03,  -2.52298499e-03,  -5.68555726e-04,\n",
       "         4.50787926e-03,   3.01287253e-03,   2.08228198e-03,\n",
       "        -1.83358591e-03,   4.72083036e-03,   2.74113007e-03,\n",
       "         3.50522995e-03,   1.01302611e-03,   3.73726034e-05,\n",
       "         4.07041563e-03,   2.94408435e-03,  -3.93566815e-03,\n",
       "         2.02977704e-03,  -1.57497858e-03,   3.15927551e-03,\n",
       "         2.78623472e-03,  -2.12867069e-03,   3.89312417e-03,\n",
       "        -2.59898813e-03,   4.71378770e-03,  -2.40649492e-03,\n",
       "        -2.94043100e-03,  -4.60899062e-03,   3.43778287e-03,\n",
       "         1.75579291e-04,   4.86881146e-03,   9.25665605e-04,\n",
       "        -4.71327599e-04,  -2.35424144e-03,  -9.14466800e-05,\n",
       "        -6.16523030e-04,  -2.68417108e-03,  -3.34157282e-03,\n",
       "         1.22811506e-03,   2.72780261e-03,   4.33614990e-03,\n",
       "        -1.18811009e-03,  -3.16246366e-03,   4.78250254e-03,\n",
       "        -3.72788543e-03,  -2.56863399e-03,   3.05323116e-03,\n",
       "         2.53991433e-03,   2.65192240e-03,   5.27790864e-04,\n",
       "        -1.09378155e-03,  -1.95310637e-03,   4.35105851e-03,\n",
       "        -1.01290492e-03,   4.95077018e-03,   1.38451683e-03,\n",
       "         2.46174377e-03,  -1.08758151e-03,  -1.27511215e-03,\n",
       "        -1.86449010e-03,  -1.69579673e-03,   3.59316799e-03,\n",
       "         2.63352063e-03], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['first']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3590976094340832"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5603/15603"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
