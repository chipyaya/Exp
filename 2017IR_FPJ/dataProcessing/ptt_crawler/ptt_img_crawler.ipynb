{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "29\n",
      "40\n",
      "28\n",
      "39\n",
      "25\n",
      "0\n",
      "32\n",
      "unknown url type: '//farm5.staticflickr.com/4219/35007622745_df5f644669_b.jpg'\n",
      "44\n",
      "2\n",
      "28\n",
      "0\n",
      "0\n",
      "1\n",
      "unknown url type: '//farm5.staticflickr.com/4234/34499970004_120f098a88_b.jpg'\n",
      "1\n",
      "unknown url type: '//farm5.staticflickr.com/4234/34499970004_120f098a88_b.jpg'\n",
      "37\n",
      "22\n",
      "25\n",
      "1\n",
      "12\n",
      "36\n",
      "13\n",
      "unknown url type: '//farm1.staticflickr.com/669/23290562251_49b72a4650_z.jpg'\n",
      "24\n",
      "29\n",
      "0\n",
      "14\n",
      "0\n",
      "0\n",
      "22\n",
      "17\n",
      "0\n",
      "23\n",
      "1\n",
      "unknown url type: '//farm5.staticflickr.com/4234/34499970004_120f098a88_b.jpg'\n",
      "36\n",
      "26\n",
      "30\n",
      "47\n",
      "21\n",
      "39\n",
      "2\n",
      "0\n",
      "36\n",
      "1\n",
      "36\n",
      "22\n",
      "36\n",
      "23\n",
      "27\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "import os.path\n",
    "import urllib.request\n",
    "\n",
    "ptt_url = 'https://www.ptt.cc'\n",
    "def get_webpage(url):\n",
    "    resp = requests.get(url)\n",
    "    resp.encoding = 'utf-8'\n",
    "    if resp.status_code != 200:\n",
    "        print('Invalid url:', resp.url)\n",
    "        return None\n",
    "    else:\n",
    "        return resp.text\n",
    "\n",
    "def parse_post(webpage):\n",
    "    soup = BeautifulSoup(webpage, 'html.parser')\n",
    "    links = soup.find(id='main-content').find_all('a')\n",
    "    img_urls = []\n",
    "    blog_url = ''\n",
    "    for link in links:\n",
    "        if re.match(r'^.*jpg', link['href']):\n",
    "            img_urls.append(link['href'])\n",
    "        elif re.match(r'^https?://(i.)?(m.)?imgur.com', link['href']):\n",
    "            img_urls.append(link['href'])\n",
    "        elif 'pixnet.net' in link['href']:\n",
    "            blog_url = link['href']\n",
    "    return img_urls, blog_url\n",
    "\n",
    "def save(img_urls, dir_name):\n",
    "    try:\n",
    "        if not os.path.isdir(dir_name):\n",
    "            os.mkdir(dir_name)\n",
    "        index = 0\n",
    "        for img_url in img_urls:\n",
    "            if re.match(r'^https?://(i.)?(m.)?imgur.com', img_url):\n",
    "                if img_url.split('//')[1].startswith('m.'):\n",
    "                    img_url = img_url.replace('//m.', '//i.')\n",
    "                if not img_url.split('//')[1].startswith('i.'):\n",
    "                    img_url = img_url.split('//')[0] + '//i.' + img_url.split('//')[1]\n",
    "                if not img_url.endswith('.jpg'):\n",
    "                    img_url += '.jpg'\n",
    "            fname = f'{index}.jpg'\n",
    "            index += 1\n",
    "            urllib.request.urlretrieve(img_url, os.path.join(dir_name, fname))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "\n",
    "def get_imgs_blog(webpage):\n",
    "    soup = BeautifulSoup(webpage, 'html.parser')\n",
    "    content = soup.find('div', {'class': 'article-content'})\n",
    "    \n",
    "    imgs = content.find_all('img')\n",
    "    img_urls = []\n",
    "    for img in imgs:\n",
    "        img_url = img['src']\n",
    "        if img_url.endswith('.jpg'):\n",
    "            img_urls.append(img_url)\n",
    "    return img_urls\n",
    "     \n",
    "def main():\n",
    "    ptt_dir = '/tmp2/GorsachiusMelanolophus/ptt_posts_new/sponsored/'\n",
    "    imgs_dir = '/tmp2/GorsachiusMelanolophus/ptt_imgs/sponsored/'\n",
    "    blog_count = 0\n",
    "    inpost_count = 0\n",
    "    for i in range(10000):\n",
    "        try:\n",
    "            if i % 100 == 0:\n",
    "                print(i)\n",
    "            post_path = ptt_dir + str(i) + '.p'\n",
    "            post = pickle.load(open(post_path, 'rb'))\n",
    "            url = ptt_url + post['href']\n",
    "            webpage = get_webpage(url)\n",
    "            imgs, blog_url = parse_post(webpage)\n",
    "            if imgs:\n",
    "                inpost_count += 1\n",
    "                save(imgs, imgs_dir + str(i))\n",
    "            if blog_url:\n",
    "                blog_count += 1\n",
    "                webpage = get_webpage(blog_url)\n",
    "                imgs = get_imgs_blog(webpage)\n",
    "                print(len(imgs))\n",
    "                save(imgs, imgs_dir + str(i))\n",
    "        except KeyboardInterrupt:\n",
    "            return 0\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "    print('blog_count: '+str(blog_count))\n",
    "    print('inpost_count: '+str(inpost_count))\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
